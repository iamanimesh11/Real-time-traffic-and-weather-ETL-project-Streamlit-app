# ğŸš¦ Real-Time  ETL of Road Traffic & Weather Monitoring.

> ğŸ›°ï¸ An end-to-end real-time data engineering pipeline to collect, process, and visualize road traffic & weather data using **Kafka**, **Airflow**, **PostgreSQL**, and **Grafana Loki**â€”fully containerized with **Docker**.

---
**Remarks**:  
In real-world Data Engineering projects, deploying a full-scale production setup can be costly. Therefore, for the purpose of showcasing, the entire infrastructure in this project is built and demonstrated locally using Docker â€” ensuring it's fully reproducible without incurring any extra cost.

---

## ğŸ“š Table of Contents

- [Key Features](#-key-features)
- [Tech Stack](#-tech-stack)
- [Getting Started](#-getting-started)
- [Prerequisites](#-prerequisites)
- [Architecture](#architecture)
- [Setup Instructions](#setup-instructions)
- [Directory Structure](#-directory-structure)
- [Configurations](#configurations)
- [Logging & Monitoring](#-logging--monitoring)
- [Screenshots](#screenshots)
- [Future Scope](#-future-scope)
- [Author](#-author)


## ğŸ”‘ Key Features



- **ğŸ³ Fully Dockerized Architecture**  
  Deploy the entire stack with a single `docker-compose up --build` â€” no manual setup.

- **âš™ï¸ Real-Time ETL Pipeline with Kafka Streaming**  
  Data is streamed in real-time using Apache Kafka, then processed via Python-based ETL jobs and stored in PostgreSQL.

- **â° Airflow-Based Workflow Orchestration**  
  Apache Airflow schedules and manages ETL workflows and task dependencies.
  
- **ğŸ“¡ Apache Kafka for High-Throughput Streaming**  
  Handles real-time data ingestion and decoupling between data producers and consumers.

- **ğŸ“ Centralized Logging with Loki**  
  All logs from Python apps and Airflow tasks are sent to Grafana Loki for monitoring and troubleshooting.

- **ğŸ“Š Visual Monitoring with Grafana**  
  Dashboards offer real-time insights into pipeline performance, traffic flow, and logs.

- **ğŸ”” Notification System (Optional)**  
  Sends ETL job alerts (success/failure) via Discord webhooks.

- **ğŸ” Secure Credential & API Key Management**  
  Firebase securely stores API keys, secrets, and credentials â€” no hardcoding.

- **ğŸ’¾ Persistent PostgreSQL Storage**  
  Maintains structured data and ensures durability across restarts.

- **ğŸ“ Configurable & Extensible**  
  Clean modular structure with support for external config files, secrets, and new data sources.

- **ğŸ‘¨â€ğŸ’» Plug-and-Play for Recruiters**  
  Instantly clonable and runnable â€” ideal for technical demos or code evaluations.

---
#ğŸ› ï¸Tech Stack

| Component      | Tool / Service        | Logo                              |
|----------------|-----------------------|-----------------------------------|
| **Data Source** | TomTom, Overpass, WeatherAPI | <img src="https://upload.wikimedia.org/wikipedia/commons/c/c1/Tomtom_logo.jpg" alt="TomTom" width="50"/> <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Openstreetmap_logo.svg/225px-Openstreetmap_logo.svg.png" alt="Overpass" width="50"/> <img src="https://openweathermap.org/themes/openweathermap/assets/img/logo_white_cropped.png" alt="WeatherAPI" width="50"/> |
| **Scheduler**  | Apache Airflow         | <img src="https://icon.icepanel.io/Technology/svg/Apache-Airflow.svg" alt="Airflow" width="70"/> |
| **Streaming**  | Apache Kafka           | <img src="https://irisidea.com/wp-content/uploads/2024/04/kafka-implementation-experience--450x231.png" alt="Kafka" width="120"/> |
| **Storage**    | PostgreSQL             | <img src="https://www.logo.wine/a/logo/PostgreSQL/PostgreSQL-Logo.wine.svg" alt="PostgreSQL" width="120"/> |
| **Logging**    | Grafana Loki           | <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a1/Grafana_logo.svg/2005px-Grafana_logo.svg.png" alt="Grafana Loki" width="100"/> |
| **UI framework**    | Streamlit           | <img src="https://streamlit.io/images/brand/streamlit-logo-primary-colormark-darktext.png" alt="Streamlit" width="180"/> |
| **Containerization**  | Docker, Docker Compose | <img src="https://cdn4.iconfinder.com/data/icons/logos-and-brands/512/97_Docker_logo_logos-1024.png" alt="Docker" width="100"/>|
| **API & Credentials**   | Firebase               | <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTxQktpK3Jy3GkxXutGPzl8R3OBCNMxfFWP5A&s" alt="Firebase" width="130"/>|
| **Alerts and other**   | Discord               | <img src="https://pngimg.com/uploads/discord/discord_PNG3.png" alt="Discord" width="110"/>|
| **Language**   | Python                 | <img src="https://s3.dualstack.us-east-2.amazonaws.com/pythondotorg-assets/media/community/logos/python-logo-only.png" alt="Python" width="70"/>|

---

# Architecture




# ğŸš€ Getting Started

## âš ï¸ IMPORTANT

<p align="left">
  <img src="https://img.freepik.com/free-vector/www-concept-illustration_114360-2143.jpg?t=st=1744565213~exp=1744568813~hmac=cc0420ee0ca016a8962950768146a9a73c652ef7e93dfd0f6be86f2c3eca7cb6&w=826" alt="API Status" width="200"/>
</p>

> Before cloning and running this project, **please ensure that the API is up and running.**

ğŸ”— **[Visit Project App](https://traffic-api-status.vercel.app/?show_api=1#)**  
Make sure the API is **not down** before proceeding.

ğŸ’¡ Once the API is confirmed to be **live and functional**, you can go ahead and clone the repo, and run the project locally. 
Just follow the steps below ğŸ‘‡


## âœ… Prerequisites

Before running this project locally, make sure you have the following installed on your system:

- [Docker](https://www.docker.com/products/docker-desktop) & [Docker Compose](https://docs.docker.com/compose/)
- [Git](https://git-scm.com/downloads)
- A code editor like [VS Code](https://code.visualstudio.com/)
- Internet connection to access external APIs (TomTom, WeatherAPI, etc.)

ğŸ’¡ **Note:**  
Ensure that your systemâ€™s firewall or antivirus isnâ€™t blocking Docker containers from making network requests.


# Setup Instructions


###  Clone the Repository

First, clone the repository to your local machine:

```bash
git clone https://github.com/animesh11singh/project_real_time_trafic_monitoring.git
cd project_real_time_trafic_monitoring
```
### Run in terminal

```bash
docker-compose up -d --build
```
## ğŸ“‚Directory Structure

```bash
.
â”œâ”€â”€ airflow/                 # DAGs & Airflow configs
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ traffic_pipeline_dag.py
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ producer.py          # Traffic/Weather fetch
â”‚   â””â”€â”€ consumer.py          # DB insertion
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ get_nearby_roads.py  # Overpass API call
â”‚   â”œâ”€â”€ transform_traffic.py
â”‚   â””â”€â”€ bulk_insert.py
â”œâ”€â”€ grafana_loki/
â”‚   â””â”€â”€ loki-config.yaml
â”œâ”€â”€ postgres/
â”‚   â””â”€â”€ init.sql
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md

```


### â–¶ï¸ Next Steps

Once the project is up and running, follow these steps:

1. ğŸŒ Open your browser and visit:  
   **[http://localhost:8501/](http://localhost:8501/)**  
   This will open the ETL helper streamlit app .

2. ğŸ“‹ **Follow the ETL instructions** provided on the strreamlit app to Simulate ETL step by step.

3. ğŸ³ **Keep an eye on your containers:**  
   Use `docker ps` or Docker Desktop to monitor the status of all services.

---

âœ… Everything running smoothly? You're all set to explore the project!







## Configurations 

### Access the Services
Once the containers are up and running, you can access the following services :

| Service           | URL                           | Username | Password |
|-------------------|-------------------------------|----------|----------|
| Streamlit App     | [http://localhost:8501](http://localhost:8501) | _N/A_     | _N/A_     |
| Airflow UI        | [http://localhost:8080](http://localhost:8080) | `animesh` | `animesh16` |
| Grafana Dashboard for logs | [http://localhost:3000](http://localhost:3000) | `admin`   | `animesh16` or `admin`   |

Postgrsql Database initialized at startup of Postgresql container with default configurations.

---


## ğŸ“Š Logging & Monitoring


This project implements a **centralized logging and monitoring system** using **Grafana Loki**, ensuring transparency, debuggability, and maintainability across all services.

### Key Highlights

- **Structured Logging**  
  All Python scripts across Kafka producers/consumers, Airflow DAGs, and data pipelines generate structured logs with timestamp, service name, event type, and status.

- **Centralized Collection with Grafana Loki**  
  Logs from all services are collected and pushed to Loki using `Promtail`. These logs are accessible in real-time via **Grafana dashboards**.

- **Dockerized Monitoring Stack**  
  - `Grafana` for visualization  
  - `Loki` for log storage  
  - `Promtail` for log shipping  
  These services are configured in `docker-compose.yml` with persistent volume storage.

- **Real-Time Debugging**  
  Logs include all critical operations such as:
  - API calls (Overpass, TomTom, WeatherAPI)  
  - Kafka message flow  
  - Database operations (insert/update/failure)  
  - Retry attempts and error messages  

- **Failover & Local Storage**  
  In case of Grafana/Loki downtime, logs are safely written to local files and retried later to avoid data loss.

- **Security & Hygiene**  
  - API keys and sensitive values are **excluded from log outputs**  
  - Logs are rotated and archived periodically (based on configuration)

### Accessing Logs

1. Navigate to [http://localhost:3000](http://localhost:3000)
3. Use log query labels like `{job="airflow"}` or `{job="kafka-producer"}` to filter logs
4. Dashboard panels show service-wise activity, recent errors, and API request status

ğŸ“· **Please find sample images of dashboards and logs below**



> âœ… This setup ensures end-to-end visibility into your ETL pipeline operations.

## ğŸ—ƒï¸ Data Stored

| Table Name       | Description                      |
|------------------|----------------------------------|
| roads_traffic     | Road metadata from Overpass API |
| traffic_flow_data | Real-time traffic speed data    |
| weather_conditions| Weather data per coordinate     |

---


## ğŸ“Š Future Scope

- **Advanced Analytics & ML Integration:**  
  Implement predictive models for traffic congestion, accident risk zones, or weather-based route recommendations.

- **Real-Time Alert System:**  
  Notify users of traffic anomalies or severe weather via email, SMS, or push notifications.

- **Interactive Dashboard:**  
  Integrate tools like Streamlit or Dash for live data visualization and insights.

- **Scalability Enhancements:**  
  Deploy to cloud platforms (AWS, GCP, or Azure) using Kubernetes and CI/CD pipelines for production readiness.

- **API Gateway & Access Layer:**  
  Build secure REST APIs for external systems to query real-time traffic insights.

- **Data Lake Integration:**  
  Archive historical traffic and weather data to a data lake for long-term analysis and trend forecasting.
---

Remarks :
As we know in Data Engineering project,its impossible to bear cost of production and only way is to do everything locally for showcasing a project.


## ğŸ‘¤ Author

- **[Animesh Singh]**
- ğŸ’¼ Aspiring Data Engineer | Big Data |Python | Postgresql/Databases| Kafka | Airflow | Docker 
```

----------------------------------------------------------------------------
